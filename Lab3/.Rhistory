#which is the inverse kernel width for the gaussian kernel
#	cost of constraints violation (default: 1) this is the ‘C’-constant of the regularization term in the Lagrange formulation.
#C is what we change thoruhgout the iterations here. C = 1/2nlambda dvs för regularizationen alpha_i < 1/2nlambda
#detta är vårt C.
#dvs när i ökar, så ökar C, och detta betyder att lambda minskas.-> mer regularisering i feature space
#(ty inversen 1/lambda ökar där) -> färre suport vectors.
#medan regularization i innput space minskar ty lambda minskar.
filter <- ksvm(type~.,data=train,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
#predict on validation data, spam or nonspam
mailtype <- predict(filter,val[,-58])
#confusion matrix of predictions and true labels
t <- table(mailtype,val[,58])
#err_al = err_val + den nya typ. eftersom vi kör c(gamla, + en ny)
#misclassification. summan av diagonalen, dvs nonspam nonspam + spam spam / alla i t.
err_val <-c(err_val,(t[1,2]+t[2,1])/sum(t))
}
t
mailtype
err_val
plot(err_val)
which.min(err_val)
for(i in seq(by,5,by)){print(i)}
13*0.3
View(filter0)
View(filter)
View(filter2)
View(filter3)
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
errors
we cat("The generaliztion error is:", round(err1,4))
cat("The generaliztion error is:", round(err1,4))
cat("The generaliztion error is:", round(err1,4))
View(filter1)
filter1@coef[[1]]
filter1@prob.model[[1]]
filter1@kernelf
?alphaindex
filter3
View(filter3)
View(filter3)[1]
View(filter3)[[1]]
alphaindex(filter3)
alphaindex(filter3)[[1]]
alphaindex(filter3)
g = alphaindex(filter3)
ndim(g)
dim(g)
nrow(g)
ncol(g)
g = alphaindex(filter3)[[1]]
nrow(g)
ncol(g)
g_null = alphaindex(filter3)
g_null
g
g
gg = gnull[[1]]
gg = g_null[[1]]
?alphaindex
?alphaindex
?coef
??ksvm
?kernellab
??kernallab
??kernallab
??kernalab
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
length(supportVectors)
co
inte
inte<- b(filter3)
inte
intercept = -b(filter3)
intercept
predict(filter3,spam[1:10,-58], type = "decision")
spam[1]
spam[i,]
spam[1,]
spam[2,]
spam[3,]
spam[3,]
predict(filter3,spam[1:10,-58], type = "decision")
supportVectors[1]
supportVectors[2]
supportVectors[3]
supportVectors[4]
supportVectors[5]
supportVectors[6]
supportVectors[1]
spam[3]
dim(spam)
View(spam)
spam[3,]
spam(supportvectors[1])
spam(supportvectors[1])
spam[supportVectors[1]]
spam[supportVectors[1,]]
spam[supportVectors[1],]
spam[supportVectors[1],]
spam[3,]
spam[supportVectors[1],]
co[3]
co
k2
lincomb
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
k<-NULL
preds = NULL
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
# gg = spam[supportVectors[j] , -58] * coefs[j]
gg = 3
lincomb = lincomb + gg
k2 <- k2 + gg  # Your code here
}
k <- c(k, k2)# Your code here
preds <- c(preds, lincomb)
}
k
preds
preds = c(10)
preds
preds = rep(0,10)
preds
k<-NULL
preds = rep(0,10)
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
# gg = spam[supportVectors[j] , -58] * coefs[j]
gg = 3
lincomb = lincomb + gg
k2 <- k2 + gg  # Your code here
}
k <- c(k, k2)# Your code here
preds[i] = gg
}
k
preds
k<-NULL
preds = rep(0,10)
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
# gg = spam[supportVectors[j] , -58] * coefs[j]
gg = 3
lincomb = lincomb + gg
k2 <- k2 + gg  # Your code here
}
k <- c(k, k2)# Your code here
preds[i] = lincomb
}
k
preds
k2
testPoint
testPoint
dim(testPoint)
dim(coefs[j])
nrow(coefs[j])
ncol(coefs[j])
dim(spam[supportVectors[j] , -58])
k<-NULL
preds = rep(0,10)
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
# k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
# gg = spam[supportVectors[j] , -58] * coefs[j]
gg = 3
lincomb = lincomb + gg
#k2 <- k2 + gg  # Your code here
}
# k <- c(k, k2)# Your code here
preds[i] = lincomb
}
k
preds
#k<-NULL
preds = rep(0,10)
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
# k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
# gg = spam[supportVectors[j] , -58] * coefs[j]
gg = 3
lincomb = lincomb + gg
#k2 <- k2 + gg  # Your code here
}
# k <- c(k, k2)# Your code here
preds[i] = lincomb
}
#k
preds
preds = rep(0,10)
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
# k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
gg = spam[supportVectors[j] , -58] * coefs[j]
#gg = 3
lincomb = lincomb + gg
#k2 <- k2 + gg  # Your code here
}
# k <- c(k, k2)# Your code here
preds[i] = lincomb
}
#k
preds
dim(preds)
nrow(preds)
ncol(preds)
#k<-NULL
preds = rep(0,10)
for(i in 1:2){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
# k2<-NULL
lincomb = 0
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
gg = spam[supportVectors[j] , -58] * testPoint * coefs[j]
#gg = 3
lincomb = lincomb + gg
#k2 <- k2 + gg  # Your code here
}
# k <- c(k, k2)# Your code here
preds[i] = lincomb
}
#k
preds
xD = numeric(10)
xD
predsss = rep(0,10)
spam[supportVectors[1] , -58]
testPoint
spam[3,]
unlist(spam[3,])
?rbfdot
kernel <- function (x, xnew, sigma = 0.05) {
#euclidean distance: ||x-x*||^2 = sqrt( (x1-x1)^2* + (x2-x2)^2 * + ... (xn-xn)^2 )
kernelValue = exp(-sigma * sqrt(sum((x-xnew)^2)) )
return (kernelValue)
}
kernel(c(10,3,2),c(5,-3,2))
kernel(c(10,3,2),c(5,-3))
kernel(c(10,3),c(5,-3))
kernel(c(10),c(5))
kernel(c(10),c(9))
kernel(c(10),c(2))
kernel(c(30),c(2))
kernel2 <- rbfdot(sigma = 0.05)
kernel2(c(10,3,2),c(5,-3))
kernel2(c(10,3,2),c(5,-3,7))
kernel(c(10,3,2),c(5,-3,7))
kernel <- function (x, xnew, sigma = 0.05) {
#euclidean distance: ||x-x*||^2 = sqrt( (x1-x1)^2* + (x2-x2)^2 * + ... (xn-xn)^2 )
kernelValue = exp(-sigma * (sqrt(sum((x-xnew)^2)))^2 )
return (kernelValue)
}
kernel(c(10,3,2),c(5,-3,7))
kernel2(c(10,3,2),c(5,-3,7))
kernel2(c(10,3),c(5,-3))
kernel1(c(10,3),c(5,-3))
kernel(c(10,3),c(5,-3))
kernel <- function (x, xnew, sigma = 0.05) {
#euclidean distance: ||x-x*||^2 = sqrt( (x1-x1)^2* + (x2-x2)^2 * + ... (xn-xn)^2 )
# |x-x'|^2 --> med eucilidan distance --> sum((x-xnew)^2), dvs sqrt och ^2 tar ut varandra
kernelValue = exp(-sigma * (sqrt(sum((x-xnew)^2)))^2 )
return (kernelValue)
}
kernel2 <- rbfdot(sigma = 0.05)
#yhat(x*) = alphahat^t * K(X,x*)
#k<-NULL
preds = rep(0,10)
k <- NULL
for(i in 1:2){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
k2<-NULL
lincomb = rep(0,length(supportVectors))
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
## gg = spam[supportVectors[j] , -58] * testPoint * coefs[j]
k2 <- c(k2, kernel(spam[supportVectors[j] , -58] * testPoint))
# lincomb[j] <- kernel(spam[supportVectors[j] , -58] * testPoint)
#lincomb = lincomb + gg
#k2 <- k2 + gg  # Your code here
}
# k <- c(k, k2)# Your code here
preds[i] = lincomb
}
#k
preds
#sign of this is prediction. so negative is spam, positive nto spam right?
predict(filter3,spam[1:10,-58], type = "decision")
k2
kernel(spam[supportVectors[j] , -58] * testPoint)
kernel(spam[supportVectors[1] , -58] * testPoint)
kernel <- function (x, xnew, sigma = 0.05) {
#euclidean distance: ||x-x*||^2 = sqrt( (x1-x1)^2* + (x2-x2)^2 * + ... (xn-xn)^2 )
# |x-x'|^2 --> med eucilidan distance --> sum((x-xnew)^2), dvs sqrt och ^2 tar ut varandra
kernelValue = exp(-sigma * (sqrt(sum((x-xnew)^2)))^2 )
return (kernelValue)
}
kernel2 <- rbfdot(sigma = 0.05)
#yhat(x*) = alphahat^t * K(X,x*)
#k<-NULL
preds = rep(0,10)
k <- NULL
for(i in 1:2){ # We produce predictions for just the first 10 points in the dataset.
testPoint = spam[i,-58]
k2<-NULL
lincomb = rep(0,length(supportVectors))
#supportVectors - index av de som är supportvectors.
#spam(supportVectors[1]) - den första supportvectorn
#i vårt fall är supportVectors[1] = 3
#spam[3,] - ger oss den supportVector, dvs test pointen, och dess värden för de 58 features
#DVS samma som att ta spam[supportVectors[1], ]
#sen även ta bort -58.
for(j in 1:length(supportVectors)){
#LK mellan vår point x* och våra supportvectors + intercept?
#vår rad[kolumn]*varje support vector / summan? för varje rad, + intercept ?
## gg = spam[supportVectors[j] , -58] * testPoint * coefs[j]
k2 <- c(k2, kernel(spam[supportVectors[j] , -58], testPoint))
# lincomb[j] <- kernel(spam[supportVectors[j] , -58], testPoint)
#lincomb = lincomb + gg
#k2 <- k2 + gg  # Your code here
}
# k <- c(k, k2)# Your code here
preds[i] = lincomb
}
#k
preds
#sign of this is prediction. so negative is spam, positive nto spam right?
predict(filter3,spam[1:10,-58], type = "decision")
k2
k2
sum(k2)
coefs
coefs[3]
coefs[2]
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
k2
lincomb
k2[1:10]
lincomb[1:10]
sum(k2)
sum(lincomb)
k
preds
sum(lincomb)
intercept
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
preds
all(preds==predict(filter3,spam[1:10,-58], type = "decision"))
predict(filter3,spam[1:10,-58], type = "decision")
unlist(predict(filter3,spam[1:10,-58], type = "decision"))
unlist(k)
typeof(k)
typeof(predict(filter3,spam[1:10,-58], type = "decision"))
SVMmodelPredictions = predict(filter3,spam[1:10,-58], type = "decision")
View(SVMmodelPredictions)
SVMmodelPredictions
as.numeric(SVMmodelPredictions)
as.numeric(SVMmodelPredictions) = predict(filter3,spam[1:10,-58], type = "decision")
SVMmodelPredictions = predict(filter3,spam[1:10,-58], type = "decision")
SVMmodelPredictions = as.numeric(SVMmodelPredictions)
SVMmodelPredictions
k
all(k==SVMmodelPredictions)
identical(k,SVMmodelPredictions)
which(k != SVMmodelPredictions)
foo <- function(A,B){
if (!isTRUE(all.equal(A,B))){
mismatches <- paste(which(A != B), collapse = ",")
stop("error the A and B does not match at the following columns: ", mismatches )
} else {
message("Yahtzee!")
}
}
foo(k, SVMmodelPredictions)
all.equal(SVMmodelPredictions,k)
all(k==SVMmodelPredictions)
preds
#sign of this is prediction. so negative is spam, positive nto spam right?
SVMmodelPredictions = predict(filter3,spam[1:10,-58], type = "decision")
SVMmodelPredictions = as.numeric(SVMmodelPredictions)
all.equal(preds, SVMmodelPredictions)
cat("Does our solution and predict() return the same function?)
g
?
g
?cat
cat("Does our solution and predict() return the same function?cat("Does our solution and predict() return the same function?cat("Does our solution and predict() return the same function?
?cat
Cat("Are our above predictions from 'manual' calculations the same as using predict with our SVM model?",
all.equal(preds, SVMmodelPredictions))
cat("Are our above predictions from 'manual' calculations the same as using predict with our SVM model?",
all.equal(preds, SVMmodelPredictions))
preds[1] = 0
cat("Are our above predictions from 'manual' calculations the same as using predict with our SVM model?",
all.equal(preds, SVMmodelPredictions))
foo <- function(A,B){
if (!isTRUE(all.equal(A,B))){
mismatches <- paste(which(A != B), collapse = ",")
stop("error the A and B does not match at the following columns: ", mismatches )
} else {
message("Yahtzee!")
}
}
foo(k, SVMmodelPredictions)
foo <- function(preds,SVMmodelPredictions){
if (!isTRUE(all.equal(A,B))){
mismatches <- paste(which(A != B), collapse = ",")
stop("error the A and B does not match at the following columns: ", mismatches )
} else {
message("Yahtzee!")
}
}
foo(k, SVMmodelPredictions)
foo <- function(A,B){
if (!isTRUE(all.equal(A,B))){
mismatches <- paste(which(A != B), collapse = ",")
stop("error the A and B does not match at the following columns: ", mismatches )
} else {
message("Yahtzee!")
}
}
foo(k, SVMmodelPredictions)
foo <- function(A,B){
if (!isTRUE(all.equal(A,B))){
mismatches <- paste(which(A != B), collapse = ",")
stop("error the A and B does not match at the following columns: ", mismatches )
} else {
message("Yahtzee!")
}
}
foo(preds, SVMmodelPredictions)
source("E:/kod/TDDE01/Lab3/Lab3Block1_2021_SVMs_St (1).R", echo=TRUE)
